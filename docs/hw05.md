<p><a name=top> </a>&nbsp;</p>
<p align=center>
    <a
    href="/README.md#top"><img
    src="https://img.shields.io/badge/Home-%23ff5733?style=for-the-badge&logo=home&logoColor=white"></a> <a
    href="/docs/syllabus.md#top"><img
    src="https://img.shields.io/badge/Syllabus-%230055ff?style=for-the-badge&logo=openai&logoColor=white"></a> <a
    href="https://docs.google.com/spreadsheets/d/1Jlx-BBsvVqmWhW1L9Fz6u18vPSjGXj1i/edit?usp=sharing&ouid=110996670184359055145&rtpof=true&sd=true"><img
    src="https://img.shields.io/badge/Groups-%23ffd700?style=for-the-badge&logo=users&logoColor=white"></a> <a
    href="https://moodle-courses2425.wolfware.ncsu.edu/course/view.php?id=7150"><img
    src="https://img.shields.io/badge/Moodle-%23dc143c?style=for-the-badge&logo=moodle&logoColor=white"></a> <a
    href="https://discord.gg/whDXzJGP"><img
    src="https://img.shields.io/badge/Discord-%23008080?style=for-the-badge&logo=discord&logoColor=white"></a> <a
    href="https://ncsu.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx?folderID=958aa5e8-f99e-441f-a545-b26400dfe515"><img
    src="https://img.shields.io/badge/Videos-%23ffa500?style=for-the-badge&logo=youtube&logoColor=white"></a> <a
    href="/LICENSE.md"><img
    src="https://img.shields.io/badge/(c)%20Tim%20Menzies,%202025-%234b4b4b?style=for-the-badge&logoColor=white"></a>
    <br>&nbsp;<br>
    <img width=200 src="/img/banner2.png">
</p>
<h1 align="center">:cyclone:&nbsp;CSC510: Software Engineering<br>NC&nbsp;State, Spring&nbsp;'25</h1>
      



Our notes here come from [Software Carpentary](https://swcarpentry.github.io/make-novice/index.html) notes
and the amazing [MIT missing semester](https://missing.csail.mit.edu/2020/metaprogramming/)
subject. 


As such, we share their copyright:
[Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International](https://github.com/idleberg/Creative-Commons-Markdown/blob/main/4.0/by-nc-sa.markdown),


## Introduction


Make is a tool which can run commands to read files, process these files in some way, and write out the processed files. For example, in software development, Make is used to compile source code into executable programs or libraries, but Make can also be used to:


- run analysis scripts on raw data files to get data files that summarize the raw data;
- run visualization scripts on data files to produce plots; and to
-  parse and combine text files and plots to create papers.
- record, in an executable form, all your short cuts 


Make is called a build tool - it builds data files, plots, papers, programs or libraries. It can also update existing files if desired.


Make tracks the dependencies between the files it creates and the files used to create these. If one of the original files (e.g. a data file) is changed, then Make knows to recreate, or update, the files that depend upon this file (e.g. a plot).


Make was first released in April 1977 by Stuart Feldman at Bell Labs. That makes it nearly 50 years oldâ€”but it's still widely used since:


- **Ubiquity & Stability** â€“ *Make is deeply embedded in Unix-based systems, well-documented, and universally supported.*
- **Minimal Overhead** â€“ *Newer tools (CMake, Ninja, Bazel) can be heavier, requiring extra dependencies or complex setup.*
- **Declarative & Efficient** â€“ *It tracks dependencies automatically, only rebuilding what's necessary, without overengineering.*
- **Works Everywhere** â€“ *Unlike newer tools, Make runs on virtually any system without special installation or configuration.*


Make was  motivated by a need for tool to automate software compilation dependenciesâ€”a problem that was becoming more complex as projects grew. The Make work laid the foundation for modern build automation and dependency management.


That said, if you're dealing with massive parallel builds, cross-platform projects, or non-C/C++ languages, newer tools might be a better fit. e.g.


- **CMake** â€“ Meta-build system that generates Makefiles or Ninja files, common for C/C++.  
- **Ninja** â€“ Fast, minimal build system, often used with CMake.  
- **Meson** â€“ Simpler, faster alternative to CMake, optimized for performance.  
- **Bazel** â€“ Scalable build tool from Google, great for large, multi-language projects.  
- **Gradle** (Java/Kotlin) â€“ Powerful build automation, used for Android development.  
- **Cargo** (Rust) â€“ Rustâ€™s built-in build system and package manager.  
- **Maven** (Java) â€“ Dependency management and build tool for Java applications.  
- **SBT** (Scala/Java) â€“ Optimized for Scala, supports parallel execution.  
- **Just** â€“ Modern, simple task runner, like Make but with cleaner syntax.  
- **Taskfile** â€“ YAML-based task runner, popular in the Go ecosystem.  
- **Rake** (Ruby) â€“ Rubyâ€™s Make-like tool for automation and builds.  
- **Leiningen** (Clojure) â€“  manages dependencies and tasks.


## Key Makefile Concepts
A Makefile consists of **rules**, **targets**, **dependencies**, and **commands**. The basic structure is:


```
target: dependencies
	command
```


- **target**: The output file or action.
- **dependencies**: Files required to produce the target.
- **command**: The shell command to generate the target.


By default, Make runs a `Makefile` of commands in the local directory (but this can be reset using make -f file.mk).


Example


```make
updata.txt: data.txt
   echo "uping..."
   gawk '{print toupper($$0)}' data.txt > updata.txt
```


> Note that
all command lines _**must**_ start with a tab.
Aso, `$$` is the Make variable that returns `$`
    (so `$$0` is the gawk variable `$0` 
     but `$(SHELL)` is a make variable


Run with:


```sh
make updata.txt
```


In this example?


```
Makefiles are traditionally used for compiling code, but they can
also be powerful tools for automating data-driven workflows. This
tutorial focuses on using make alongside gawk, grep, and sed to
process data efficiently.
```


is processed and saved eslwhere as


```
MAKEFILES ARE TRADITIONALLY USED FOR COMPILING CODE, BUT THEY CAN
ALSO BE POWERFUL TOOLS FOR AUTOMATING DATA-DRIVEN WORKFLOWS. THIS
TUTORIAL FOCUSES ON USING MAKE ALONGSIDE GAWK, GREP, AND SED TO
PROCESS DATA EFFICIENTLY.
```
Note that the commands are only run on the dependencies are new than than the target.


- So if we run this twice, we only see the echo once.


```sh
$ make -s updata.txt
uping...


$ make -s  updata.txt
$
```


    
>  Note that `make -s` siences verbose output. Don't use it during debugging (when youw ant to see those errors)


### Automatic Variables


Make provides several automatic variables that simplify rule definitions:


- `$@` - The target filename.
- `$<` - The first prerequisite (dependency).
- `$^` - All prerequisites.


Example:


```make
%.uppercase: %
	gawk '{print toupper($0)}' $< > $@
```


Run:


```sh
make file.txt.uppercase
```


### Wildcard and Pattern Matching


```make
DATAFILES := $(wildcard dataset/*.txt)
PROCESSED := $(patsubst dataset/%, processed/%, $(DATAFILES))


processed/%.txt: dataset/%.txt
	gawk '{print toupper($0)}' $< > $@
```


### Functions in Makefiles


```make
EXT = txt
FILES = $(wildcard *.$(EXT))


print_files:
	@echo $(FILES)
```


### Self-Documenting Makefiles


```make
.PHONY: help
help : Makefile ## print help
	echo ""; printf "usage: make [OPTIONS]\n\n"
	@gawk 'BEGIN {FS="[ \t]*:.*##[ \t]*"}  \
	  NF==2 { printf \
           "  \033[36m%-25s\033[0m %s\n","make " $$1,$$2}'  $< \
	| grep -v awk
```


@: run without echoding line
\: used to continue previous line.
   - Pro-tip each line in the command are a _seperate_ shell environment
     (unless you continue them)


Run:


```sh
make help
```


### Short cuts


```
LOUD = \033[1;34m#
SOFT = \033[0m#


push: ## commit to main
	- echo -en "$(LOUD)Why this push? $(SOFT)" ;  read x ; git commit -am "$$x" ;  git push
	- git status
```


### Reproducaibilityity


```
exp1: ../../moot/optimize/[chmp]*/*.csv
	$(foreach f, $^, (lua kah.lua --comparez $f & ); )


experiment1:; $(MAKE) exp1 | tee ~/tmp/$@.out
```


- Output goes to ~/tmp/experiment1.out (and gets copied to screen along the way)- Note the dependencies with wildcards.
   - This four lines run over dozens of  data sets
      [stored in the subdirectories of ../moot/optimizer/...](https://github.com/timm/moot/tree/master/optimize).
- Note the run in background command "&"
   - These four lines set up dozens of parralle tasks. Brings your computer to its knees (for a while).
    


## Data Processing with `gawk`, `grep`, and `sed`


### Extracting Data with `grep`


```make
filter: dataset/data.txt
	grep "pattern" dataset/data.txt > filtered.txt
```


### Processing with `gawk`


```make
analyze: dataset/data.txt
	gawk '{sum += $$2} END {print "Total:", sum}' dataset/data.txt > summary.txt
```


### Stream Editing with `sed`


```make
transform: dataset/data.txt
	sed 's/old/new/g' dataset/data.txt > transformed.txt
```


## Homework: Text Processing with Make

### What to Hand In
Submit a link to a directory in a repository containing:
- All your data and scripts
- The following output files:
  - `step1.txt`
  - `step2.txt`
  - `step3.txt`
  - `step4.txt`
 
### Submitting Your Work
Create a PDF file containing:
- Your repository link
- Group number
- Names of all participants (Format: `Name - GitHub User ID`)

ðŸ“Œ **Important:** Only one group member submits the work.

### Assignment Details
The file available at [data.txt](https://github.com/txt/se25/blob/main/docs/data.txt) contains approximately 50 paragraphs.

Your task is to:
1. **Clean the file**: Remove punctuation and convert text to lowercase.
2. **Remove stop words**: Eliminate common stop words from the text.
3. **Identify the ten most frequent words**: Find the ten most used terms in the text.
4. **Generate a word frequency table**: Create a new file with one line per paragraph, showing the frequency counts of the top ten terms in each paragraph.

For example, if the most frequent words are:
```
challenging,do,key,and,data,example,aspect,engineering,we,
```
and in the first paragraph `challenging` and `aspect` appear once, while `data` appears three times, the output should look like:
```
challenging,do,key,and,data,example,aspect,engineering,we,
1,0,0,0,3,0,1,0,0,
0,1,1,2,1,1,0,0,1,
2,0,1,4,3,2,0,0,0,
0,0,1,1,1,1,2,0,3,
1,1,2,2,1,1,0,2,5,
3,2,1,1,4,0,0,1,1
...
```

### Makefile Instructions
You are provided with a Makefile to automate the process. You need to write killstopXXX.awk, a magic piece of scripting at YYY and ZZZ.awk



#### Makefile Code:
```make
# Define variables
INPUT = data.txt
CLEANED = cleaned.txt
STOPPED = stop.txt
TOKENS = tokens.txt
FREQS = word_counts.txt
TOP_WORDS = top.txt
TABLE = table.txt

# Run the full pipeline
all: $(TABLE)

# Step 1: Canonicalization (Kill punctuation, Lowercase, Remove Punctuation)
$(CLEANED): $(INPUT)
	sed 's/[^a-zA-Z ]//g' $< | tr 'A-Z' 'a-z' > $@

# Step 2: Remove stop words
$(STOPPED) : $(CLEANED)
	gawk -f killstopXXX.awk $< > $@

# Step 3: Report frequency of words
$(FREQS): $(STOPPED)
	cat $< | YYY | sort -nr > $@

# Step 4: Extract Top 10 most frequent words
$(TOP_WORDS): $(FREQS)
	gawk '$$2 && NR <=10 {print $$2}' $< > $@

# Step 5: Generate table of word frequencies per paragraph
$(TABLE): $(CLEANED) $(TOP_WORDS)
	gawk -f ZZZ.awk PASS=1 $(TOP_WORDS) PASS=2 $(CLEANED) > $@

# Cleanup
clean:
	rm -f $(CLEANED) $(TOKENS) $(FREQS) $(TOP_WORDS) $(TABLE)

step1:
	$(MAKE) clean $(CLEANED); head $(CLEANED)

step2:
	$(MAKE) clean $(STOPPED); head $(STOPPED)

step3:
	$(MAKE) clean $(TOP_WORDS); head $(TOP_WORDS)

step4:
	$(MAKE) clean $(TABLE); head $(TABLE) # | column -s, -t
```

(Aside: your system may not have `column` so that is commented
on on the last line. But if you have `column`, then you get a nice
pretty-print.)

#### Makefile Tasks:
- **Step 1: Text Cleaning**
  - Removes punctuation and converts text to lowercase.
  - Run using: `make step1`
- **Step 2: Stop Word Removal**
  - Eliminates common stop words.
  - Run using: `make step2`
- **Step 3: Identify Frequent Words**
  - Extracts the top ten most frequent words.
  - Run using: `make step3`
- **Step 4: Generate Word Frequency Table**
  - Counts occurrences of top words in each paragraph.
  - Run using: `make step4`


### Running Your Code

The beauty of all this is that you can test each piece.
To reset everything and just generate the `cleaned.txt` file:
```sh
make step1
```
To test the final output:
```sh
make step4
```

### Grading Scale
Total: **1 point**.
- Each task is worth **0.25 points**.
- Ensure all steps are completed to receive full credit.

