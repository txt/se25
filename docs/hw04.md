<p><a name=top> </a>&nbsp;</p>
<p align=center>
    <a
    href="/README.md#top"><img
    src="https://img.shields.io/badge/Home-%23ff5733?style=for-the-badge&logo=home&logoColor=white"></a> <a
    href="/docs/syllabus.md#top"><img
    src="https://img.shields.io/badge/Syllabus-%230055ff?style=for-the-badge&logo=openai&logoColor=white"></a> <a
    href="https://docs.google.com/spreadsheets/d/1Jlx-BBsvVqmWhW1L9Fz6u18vPSjGXj1i/edit?usp=sharing&ouid=110996670184359055145&rtpof=true&sd=true"><img
    src="https://img.shields.io/badge/Groups-%23ffd700?style=for-the-badge&logo=users&logoColor=white"></a> <a
    href="https://moodle-courses2425.wolfware.ncsu.edu/course/view.php?id=7150"><img
    src="https://img.shields.io/badge/Moodle-%23dc143c?style=for-the-badge&logo=moodle&logoColor=white"></a> <a
    href="https://discord.gg/whDXzJGP"><img
    src="https://img.shields.io/badge/Discord-%23008080?style=for-the-badge&logo=discord&logoColor=white"></a> <a
    href="https://ncsu.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx?folderID=958aa5e8-f99e-441f-a545-b26400dfe515"><img
    src="https://img.shields.io/badge/Videos-%23ffa500?style=for-the-badge&logo=youtube&logoColor=white"></a> <a
    href="/LICENSE.md"><img
    src="https://img.shields.io/badge/(c)%20Tim%20Menzies,%202025-%234b4b4b?style=for-the-badge&logoColor=white"></a>
    <br>&nbsp;<br>
    <img width=200 src="/img/banner2.png">
</p>
<h1 align="center">:cyclone:&nbsp;CSC510: Software Engineering<br>NC&nbsp;State, Spring&nbsp;'25</h1>
      



# Data Wrangling

Data wrangling, also known as data munging, is the process of cleaning, transforming, and organizing raw data into a more usable format for analysis or reporting. This process is essential because raw data is often messy, incomplete, or unstructured, making it difficult to derive meaningful insights without proper preparation. Often times, we find ourselves handling complex data that needs to be incrementally broken down into different shapes and forms at different stages to finally come off as meaningful information. Data wrangling enables us to break down a complex data processing task into several subtasks, carry out those subtasks with off-the-shelf tools, and seamlessly integrate the subtasks into a final comprehensive piece of information. This tutorial will teach us command-line tools to facilitate data wrangling.


You can follow this tutorial using Linux or MacOS terminals. If you are on Windows, you can use WSL (Windows Sub-system for Linux) command line.


## Pipes


Doug McIlroy, Bell Labs’ “Computing Techniques
Research Department” 1986

-   “We should have some ways of coupling programs like garden hose.”
- "Let programmers screw in another segment when it becomes necessary
to massage data in another way.”
- “Expect the output of
 every program to become the input to another, as yet unknown,
 program. Don’t clutter output with extraneous information.”


Pipes changed the whole idea of UNIX:


- Implemented in 1973 when ("in one feverish night",
wrote McIlroy) by  Ken Thompson.
- “It was clear to everyone, practically minutes after the system
came up with pipes working, that it was a wonderful thing. Nobody
would ever go back and give that up if they could.”
- The next
day", McIlroy writes, "saw an unforgettable orgy of one-liners
as everybody joined in the excitement of plumbing."


Pipes changed how we think about software


- Not applications
- But tools.
  - Compose the things together, if you
had made them so that they actually worked together.
  - People went back and consciously put into programs the idea that they read from a list of files
  -  And  if there were no files they read from the standard input so that they could be used in pipelines.


You've probably all used pipes already. Here we download an install shell script  then pipe it to the `sh` command
(so it runs)


```
curl -s http://example.com/install.sh | sh
```


(By the way, that is a security risk. [Use with care!](https://sysdig.com/blog/friends-dont-let-friends-curl-bash/)


Examples

- `nroff` is a text formatter (old version of latex)
- `lp` is the line printer
- `col` handles escape sequences that sets up text in 2 columns
- `tbl` is an nroff pre-processor that expresses tables in terms of lower-level nroff commands
- `eqn` is an nroff pre-processor that expresses equations in terms of lower-level nroff commands


```bash
nroff files | col | lp                    # sent to printing after adding columns
tbl file-1 file-2 . . . | eqn | nroff -ms # enable equations in files
````


Think of pipes as `produce`, `translate`, `filter`, `consume`:


```bash
find /usr/bin/ |                #produce
sed 's:.*/::'  |                #translate: strip directory part
grep -i '^z'   |                #filter   : select items starting with z
xargs -d '\n' aFinalConsumer    #consume
```


Note that in the above, the `filters` inside the `pipes` could be written in any language at all, by different teams.


Also, operating systems _love_ pipes since they can run each part of the process as a separate process,
maybe even on different machines (\*)


(\*) That said, there is the data bus / network cost of streaming data between parts of the pipe.


Problems with pipes:


- Assumes a one-way flow, here to there
  - Not ideal for GUI envrionments where users want to call functions in any order
  - Which makes pipes are more an "under the hood" idea
- Also, I've had problems with pipes and concurrency: sending data off on long retrevial arcs (e.g. from another part of the network)
  - But that's just me


Pipes are a fundamental inter-process communication mechanism on operating systems. They enable the output of one process to be redirected to another. When you write a command on the terminal, the shell is actually creating a process that can execute your command. Since pipes are used to glue together different processes, you can also use pipes on the terminal to enable communication between different commands. The pipe command, represented as a bar symbol **|**, lets you transfer the output of one command as the input of the next. To understand pipes, first let's review some handy shell commands. Feel free to open your command line or terminal. Type in the following commands and observe the output:

## Piped Commands

1. **ls:** prints out the list of files and sub-directories in the present working directory
2. **ls -l:** prints out the list of files and sub-directories in the present working directory including detailed information related to the permissions, owner, size, dates etc. of each of the files.
3. **wc \<filename\>:** counts the lines, words, and characters in a file
4. **wc -l \<filename\>:** counts only the lines in a file


Note the convention:

```bash
wc  # runs a command
wc -h # shows help on that command
```

That's all fine and dandy. But, what if you want to count the number of files in the present working directory?


If you remember the definition of data wrangling, it can involve a handful of operations on a piece of data. These operations are typically done in a specific order - one after the other. So, one needs an organized and synchronous pipeline that takes as input the original piece of data and in stages transforms it into the desired piece of information. Now, we will look at how we can create a simple but powerful data pipeline in the shell environment using the pipe ( | ) command.


Type the following command and behold: `ls | wc -l`. We are now able to count the number of files in the current directory. Now we will try to understand how the pipe command works. But to understand that, we must remember that "In Unix (like systems), everything is a file". So, during the command "ls | wc -l", the shell takes the output of the command "ls" and passes that onto the command "wc -l" as input in the form of a file (remember that "wc" expects a file as an input).


Now let's try to understand some complex examples of the pipe
command. In the given hw4.zip file, take a look inside the dataset1
directory and you will see 30 files. Each of these files contains
some text. Suppose someone asks us to filter only the files containing
where "CSC510" appears three times. Then:


```bash
grep -c CSC510 file* | grep ":3" | cut -d: -f 1
```


Now, if we are asked to sort the names of these files in a descending
order based on the size of the files, we can enhance the previous
command as follows:


```bash
(grep -c CSC510 file* | grep ":3" | cut -d: -f 1 | xargs ls -l {}  \;)  | sort -rn -k
```


Additionally, when printing the names of these filtered files, if
we want to change the file name format from “file_#” to “filtered_file_#”,
we can use the following command including “sed” for the substitution:


```bash
(grep -c CSC510 file* | grep ":3" | cut -d: -f 1 | xargs ls -l {}  \;)  | sort -rn -k 5 | sed 's/file/filtered_file/'
```



Let's now try to unravel this enigma. The command `cut` extracts a specific column from a piece of data containing multiple columns separated by some field separator that is specified with the `d` flag. `xargs` takes a number of rows and runs the command that follows (in this case ls-l) on each of the rows one at a time. `sort` command sorts the list of given inputs based on some criteria (in this case, the options for sort are `k` which specifies the range of columns used for sorting, `n` which means numeric sort and `r` which means in reverse order i.e. descending order). Next, we will look at the `grep`, `sed`, and `gawk` commands in more detail.


## GREP


grep is a powerful command-line tool used for searching plain-text data sets for lines that match a regular expression. The name stands for Global Regular Expression Print. It's commonly used in Unix-based systems like Linux and macOS. Whether you’re searching for a specific string in a file, filtering output from other commands, or looking for patterns across multiple files, grep is the tool for the job.


***Basic Syntax: grep [options] pattern [file...]***


- ***pattern:*** The string or regular expression you want to search for.


- ***file:*** The file(s) where the search is performed. If no file is specified, grep searches the input from standard input (like output from another command). 


#### Common Options


-   `-E` run the extended version of `grep`. Same thing might be possible on your system using `egrep`, not `grep`.
-   `-i`: Ignore case distinctions in both the pattern and the input files.  
    Example: grep -i "hello" file.txt
-   `-r` or `-R`: Recursively search directories.  
    Example: grep -r "hello" /path/to/directory  
-   `-l`: Print only the names of files with matching lines.  
    Example: grep -l "hello" *.txt    
-   `-n`: Prefix each line of output with the line number within its file.  
    Example: grep -n "hello" file.txt    
-   `-v`: Invert the match, showing lines that do not match the pattern.  
    Example: grep -v "hello" file.txt    
-   `-c`: Print only a count of matching lines per file.  
    Example: grep -c "hello" file.txt
-   `-o`: Print only the matched parts of matching lines.  
    Example: grep -o "hello" file.txt
    
grep is extremely powerful when used with regular expressions. Regular expressions allow you to search for complex patterns. Let's go over a quick primer of regular expressions below:


### Quick Primer on Regular Expressions (Regex)


Regular expressions (regex) are sequences of characters that define search patterns, primarily used for string matching and manipulation. They are powerful tools for searching, replacing, and parsing text in many programming languages and tools like grep, sed,gawk, and editors like VSCode.


#### Basic Components of Regex


1.  **Literals**
-   These are the simplest regex elements. They match the exact characters they represent.
-   Example: cat matches "cat" in "catapult" or "scattered".
2.  **Metacharacters**
-   Metacharacters are special symbols that have specific meanings in regex.    
-   Common metacharacters: . ^ $ * + ? {} [] () \ |


#### Common Metacharacters and Their Uses


**. (Dot)**

   -   Matches any single character except a newline.    
   -   Example: c.t matches "cat", "cut", "cot".

**^ (Caret)**

   -   Matches the start of a string.   
   -   Example: ^The matches "The" only if it's at the beginning of a line.

**$ (Dollar)**

   -   Matches the end of a string.    
   -   Example: end$ matches "end" only if it's at the end of a line.
**\* (Asterisk)**

   -   Matches 0 or more occurrences of the preceding element.    
   -   Example: ca*t matches "ct", "cat", "caaaat".

**+ (Plus)**

   -   Matches 1 or more occurrences of the preceding element.  
   -   Example: ca+t matches "cat", "caaaat", but not "ct".

**? (Question Mark)**

   -   Matches 0 or 1 occurrence of the preceding element.    
   -   Example: colou?r matches both "color" and "colour".

**{} (Braces)**

   -   Matches a specific number of occurrences of the preceding element.    
   -   Example: a{3} matches "aaa".
   -   {min,}: At least min occurrences.
   -   {min,max}: Between min and max occurrences.

**[] (Square Brackets)**

   -   Matches any one of the characters inside the brackets.
   -   Example: [aeiou] matches any vowel.
   -   [0-9] matches any digit.

**() (Parentheses)**

   -   Groups parts of a regex together and captures the matched text.    
   -   Example: (ab)+ matches "ab", "abab", "ababab".
   -   Bote: use `()` for _word1 or word2 or...i_ and use `[]` for _char1 or char2 or ..._

**| (Pipe)**

   -   Acts as a logical OR between patterns.    
   -   Example: cat|dog matches either "cat" or "dog".

**\ (Backslash)**

    -   Escapes a metacharacter to treat it as a literal.    
    -   Example: \. matches a literal dot instead of any character.


#### Character Classes


-   `\d`: Matches any digit (equivalent to [0-9]).
-   `\D`: Matches any non-digit.
-   `\w`: Matches any word character (alphanumeric + underscore).
-   `\W`: Matches any non-word character.
-   `\s`: Matches any whitespace character (space, tab, newline).
-   `\S`: Matches any non-whitespace character.
    
#### Anchors


-   ^: As mentioned earlier, it matches the start of a string.
-   $: Matches the end of a string.
    
We will be using the -E flag with grep when we want to match regular expression patterns since the -E flag enables extended regular expressions to facilitate advanced patterns like alternation, repetition, and optional items. Some examples of using regular expression search patterns with grep are as follows:


1.  Find lines that start with "Error": grep -E "^Error" logfile.txt    
2.  Find lines that end with a digit: grep -E "[0-9]$" data.txt
3.  Find lines that contain either "cat" or "dog": grep -E "cat|dog" animals.txt
4.  Find lines where "error" appears exactly three times: grep -E "(error.*){3}" logfile.txt
5.  Find lines with a date in YYYY-MM-DD format: grep -E "[0-9]{4}-[0-9]{2}-[0-9]{2}" file.txt
6.  Find lines with an email address: grep -E "[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}" file.txt
7.  Find lines that do not contain the word "success": grep -E -v "success" logfile.txt
8.  Find lines with at least one whitespace character: grep -E "\s" file.txt
9.  Find lines where the word "fail" is followed by a number: grep -E "fail[0-9]" file.txt
10.  Find lines with a 5-digit zip code: grep -E "\b[0-9]{5}\b" addresses.txt
    
## SED


SED, short for Stream EDitor, is a powerful text-processing utility found in Unix and Unix-like operating systems. It is used to perform basic text transformations on an input stream (a file or input from a pipeline). SED operates by reading the input line by line, applying specified editing commands, and then outputting the modified text. It's widely used for tasks like searching, find-and-replace operations, text deletion, and insertion.


This tutorial will introduce you to the basics of SED, covering simple operations and more advanced features.


***Basic Syntax: sed 'command' input_file***


-   ***command:*** This is where you specify what operation SED should perform, such as substitution or deletion.
-   ***input_file:*** The file you want SED to process. If you omit the file, SED will read from the standard input (such as the terminal or another command's output).
    


### Common SED Commands


#### Substitution Command
The substitution command (s) is one of the most commonly used SED commands. It replaces specified text within a line.
***Example:*** To replace "apple" with "orange" in a file:


```bash
sed 's/apple/orange/' input_file
```


This command replaces the first occurrence of "apple" with "orange" in each line of the file.


#### Global Substitution
By default, SED replaces only the first occurrence of the pattern in each line. To replace all occurrences in a line, use the g flag:
***Example:*** To replace all occurrences of "apple" with "orange" in each line:


```bash
sed 's/apple/orange/g' input_file
```


#### Deleting Lines
The delete command (d) removes entire lines that match a given pattern.
***Example:*** To delete all lines containing the word "delete_me":


```bash
sed '/delete_me/d' input_file
```


This removes any line in the file that contains "delete_me".


#### Deleting Specific Text
If you only want to delete specific text within a line without removing the entire line, you can use the substitution command with an empty replacement string.
***Example:*** To remove the word "delete_me" from a line:


```bash
sed 's/delete_me//' input_file
```


This removes "delete_me" wherever it appears in a line, but leaves the rest of the line intact.


### Advanced Features
#### Addressing


You can specify which lines to apply your SED commands to by using line numbers or patterns.
***Single Line:*** To apply a command to a specific line, you can use its line number. For example, to replace "foo" with "bar" only on line 3:


```bash  
sed '3s/foo/bar/' input_file
```


***Range of Lines:*** To apply a command to a range of lines, use the format start_line, and end_line. For example, to delete lines 3 through 5:


```bash
sed '3,5d' input_file
```


***Pattern Matching:*** You can also use patterns to match lines. For example, to replace "foo" with "bar" only on lines containing "baz":


```bash
sed '/baz/s/foo/bar/' input_file
```
***Using Multiple Commands:*** You can chain multiple commands together by separating them with a semicolon or by using the -e option. For example, to replace "foo" with "bar" and then delete lines containing "delete_me":


```bash
sed 's/foo/bar/; /delete_me/d' input_file
```


Alternatively:


```bash
sed -e 's/foo/bar/' -e '/delete_me/d' input_file
```
***Writing to a New File:*** To save the output to a new file instead of displaying it on the screen, redirect the output:


```bash
sed 's/foo/bar/' input_file > output_file
```


-   Exercise: To replace the third occurrence of "foo" in each line with "bar":  
    sed 's/foo/bar/3' input_file
-   Exercise: To replace "foo" with "bar" only on lines from 10 to 20:  
    sed '10,20s/foo/bar/' input_file
    


## GAWK


GAWK is GNU Project's version of AWK. The AWK programming language was originally created in the 1970s as a powerful text-processing language used in Unix and Unix-like operating systems. GAWK is designed for pattern scanning and processing. It allows you to extract information, perform calculations, and generate formatted reports, making it an essential tool for data analysis and text manipulation.


This tutorial will cover the basics of GAWK, including how to use it for simple tasks like text extraction, as well as more advanced features like working with fields, patterns, and GAWK scripts. If you don't have gawk installed, you should be able to easily install it using your operating system's package manager (e.g. apt or brew).


***Basic Syntax: gawk 'pattern { action }' input_file***


-   ***pattern:*** The condition that GAWK tests for each line. If the pattern is true, GAWK executes the associated action.
-   ***action:*** The command(s) GAWK runs when the pattern is matched. If no action is specified, GAWK prints the entire line.
-   ***input_file:*** The file that GAWK processes. If omitted, GAWK reads from standard input.


### Working with Fields


GAWK automatically splits each line of input into fields based on a delimiter (by default, whitespace). Each field is represented by $1, $2, etc., where $1 is the first field, $2 is the second, and so on. $0 represents the entire line.


#### Extracting Specific Fields


To print specific fields from a file:


```bash
gawk '{ print $1, $3 }' input_file
```
This command prints the first and third fields from each line of the file.


#### Changing the Field Separator


If your data is separated by something other than whitespace, you can change the field separator using the -F option.


For example, if your fields are separated by commas:


```bash
gawk -F, '{ print $1, $2 }' input_file
```
This prints the first and second fields from a comma-separated file.


### Using Patterns


GAWK allows you to specify patterns that determine which lines are processed.


#### Simple Pattern Matching
To print lines that match a specific pattern:


```bash
gawk '/pattern/ { print $0 }' input_file
```
This command prints all lines containing the word "pattern".


#### Matching with Conditions
You can also use conditions to match patterns. For example, to print lines where the value in the third field is greater than 100:


```bash
gawk '$3 > 100 { print $0 }' input_file
```
This command prints any line where the third field is greater than 100.


### Performing Calculations


GAWK can perform arithmetic operations on fields, which is particularly useful for data analysis.


#### Simple Arithmetic


To calculate the sum of the values in the second and third fields:


```bash
gawk '{ print $2 + $3 }' input_file
```
This prints the sum of the second and third fields for each line.


#### Aggregating Data
To calculate the total sum of a particular field across all lines:


```bash
gawk '{ sum += $2 } END { print sum }' input_file
```
This sums the values in the second field across all lines and prints the result at the end.


### Using Built-in Variables


GAWK provides several built-in variables that are useful for more complex tasks:


-   NR: The current record (line) number.
-   NF: The number of fields in the current record.
-   FS: The input field separator.
-   OFS: The output field separator.
 
Example: To print the line number followed by the last field on each line:


```bash
gawk '{ print NR, $NF }' input_file
```


### Writing GAWK Scripts


For more complex tasks, you can write a GAWK script and store it in a file. A GAWK script file typically looks like this:


```bash
#!/usr/bin/env gawk -f  # pro-tip#1
BEGIN { FS = ","; OFS = "\t" } {
	if ($3 > 100)
	print $1, $3 * 2
}
END { print "Processing Complete" }
```
You can run the script with:


```bash
gawk -f script.awk input_file
```

or 

```bash
chmod +x script.awk
./script.awk input_file  # if you used the above pro-tip#1
```


This script sets the field separator to a comma and the output field separator to a tab. It then prints the first field and doubles the value of the third field for lines where the third field is greater than 100. Finally, it prints "Processing Complete" after processing the entire file.

Gawk scripts can contain functions and those functions can contain local variables.

```bash
# fun.awk
function mean(a,    i,mu) {
  for(i in a) mu += a[i]
  return mu/length(a) }

function normal(mu,sd,    pi) {
  pi=355/113
  return (mu ? mu : 0) + (sd ? sd : 1) * sqrt(-2*log(rand())) * cos(2*pi*rand()) }

BEGIN {
    srand(1234567891)
    for(i=1;i<=100;i++) zero[i] = normal()
    for(i=1;i<=20;i++)   a20[i] = normal(10,2)
    for(i=1;i<=200;i++)  a200[i] = normal(10,2)
    for(i=1;i<=2000;i++) a2000[i] = normal(10,2)
    printf("zero\t %8.3f \n", mean(zero))
    printf("a20\t %8.3f \n",  mean(a20))
    printf("a200\t %8.3f \n", mean(a200))
    printf("a2000\t %8.3f \n",mean(a2000))
}
```

output:
```
gawk -f fun.awk

zero	   -0.047 
a20	        9.311 
a200	    9.951 
a2000	   10.003 
```

## And, finally ...


You've probably been wondering: ***when to use grep, sed, and gawk?*** Typically, try to use **grep** for searching, **sed** for substitutions, and **gawk** for more complex tasks. The renowned computer scientist [Douglas Mcilroy](https://en.wikipedia.org/wiki/Douglas_McIlroy) who coined the concept of pipelines, recommended the following: ***don't use sed when you can use grep and don't use (g)awk when you can use sed***.


In this tutorial, we learnt about various useful shell commands to facilitate data wrangling. We have seen complex example tasks that had to be broken down into smaller subtasks thus teaching us an important organizational lesson of task decomposition. Decomposing a complex task into smaller subtasks makes debugging and team-work much more convenient since different team members can work on different parts of the pipeline and can eventually integrate their individual parts to form the whole.


## Homework


### **task1.sh**   

Create this file `infinite.sh`

```bash
#!/bin/bash
while :
do
    sleep 0
done &
```
If you run this, it will never terminate.
   Your task is to write a simple script task1.sh that
   kills `infinite.sh`. Hint: start with running `infinite.sh` then typing `ps`.

     # edit task1.sh...
     sh infinite.sh
     your script  # run ehere to see what your script produces
     kill -9 $(ps | ...) # run the same script here to see what it produces

### **task2.sh**
Run this command to get a data file

```
cat > sample.txt <<EOF
apple
banana
apple
cherry
banana
banana
date
EOF
```

- Find all lines containing "banana":
- Find lines that do not contain "banana":
-  Use `uniq` to find `uniq`ue lines (output should be the above file, with only one line each for apple, banaona, cherry, date)
- run the command `man uniq`. Use what you found to extend the `uniq` comment to count the occurange of each work
- Combine grep and `uniq` to count occurrences of lines containing "banana":

### 5.awk

 Take a look at [titanic.csv](https://raw.githubusercontent.com/datasciencedojo/datasets/refs/heads/master/titanic.csv)
   which is a dataset containing passenger details
   and their survival during the Titanic disaster.  We want you to analyze this dataset   

Display the first five lines of the file to understand its structure.

### class.awk

Count and print the number of passengers per class

### age.awk 

Compute and print the average age of passengers

### live.awk 

Report the frequencies with which different classes survived titanic

### What to hand in

Push the above scripts
   to you GitHub repository with the same badges as those from HW1.

Submit a PDF report containing the following information:

- Group number
-  Names of the group members
- GitHub link of your HW4 repository
- Output of all the above in different files le.g task1.sh goes to task1.out


## Important Links

1. [GREP examples](https://alvinalexander.com/unix/edu/examples/grep.shtml)
2. [SED examples](https://learnxinyminutes.com/docs/sed/)
3. [AWK examples](https://learnxinyminutes.com/docs/awk/)
4. [Debugging pipes with **tee**](https://www.geeksforgeeks.org/tee-command-linux-example/)
5. [Debugging regular expressions](https://regex101.com/)
6. [MIT missing semester](https://missing.csail.mit.edu/2020/data-wrangling/)


